{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM/RAG Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4509ca7c4d4f77b584291a71dbade2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "from langchain import PromptTemplate, LLMChain, HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import re\n",
    "from rag_source import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  #This is directory\n",
    "\n",
    "# Load data for the RAG (processed_data.pkl)\n",
    "with open('processed_data.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# Load the LLaMA model for text generation\n",
    "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "#llama_pipeline = pipeline(\n",
    "#    \"text-generation\", \n",
    "#    model=model_id, \n",
    "#    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "#    device_map=\"auto\",\n",
    "#    temperature=0.3,\n",
    "#    top_p=0.9,\n",
    "#    max_length=1000\n",
    "#)\n",
    "\n",
    "#llm = HuggingFacePipeline(pipeline=llama_pipeline)\n",
    "\n",
    "local_model = \"llama\"\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=local_model, #model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"auto\",\n",
    "    temperature=0.3,  # Control randomness (lower values = more focused responses)\n",
    "    top_p=0.9,  # Filter unlikely words\n",
    "    truncation=True,\n",
    "    max_length=500\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "\n",
    "# Define a PromptTemplate that accepts prompt_text as an input\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"prompt_text\"],\n",
    "    template=\"{prompt_text}\"\n",
    ")\n",
    "\n",
    "# Initialize LLMChain with the LLM and prompt template\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt_template,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Function to directly analyze the provided text prompt\n",
    "def analyze_text_prompt(query, processed_data):\n",
    "    # Pass the prompt text to LLMChain as a dictionary\n",
    "\n",
    "    prompt_text = process_query(query, processed_data, embedding_model) + \" The correct answer is, \"\n",
    "\n",
    "    inputs = {\n",
    "        \"prompt_text\": prompt_text\n",
    "    }\n",
    "    \n",
    "    # Run the LLM chain and get the result\n",
    "    result = llm_chain.run(inputs)\n",
    "\n",
    "    return prompt_text, result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:  You are an expert in computer architecture and your job is to answer questions given data from cache traces. Base your response on the following data and your knowledge of computer architecture.\n",
      "\n",
      "Answer the following question: For the cache access with PC 0x403a85 and address 0x35e798a637f on the bzip workload with PARROT replacement policy, does the cache hit or miss? The correct answer is, \n",
      "\n",
      "Response:  0x403a85 is the PC address, 0x35e798a637f is the address, and the cache hit or miss is a miss.\n"
     ]
    }
   ],
   "source": [
    "query = \"For the cache access with PC 0x403a85 and address 0x35e798a637f on the bzip workload with PARROT replacement policy, does the cache hit or miss?\"\n",
    "\n",
    "prompt, response = analyze_text_prompt(query, loaded_data)\n",
    "\n",
    "# Remove the prompt_text from the beginning of result\n",
    "if response.startswith(prompt):\n",
    "    response = response[len(prompt):]\n",
    "\n",
    "print(\"\\nPrompt: \", prompt)\n",
    "print(\"\\nResponse: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
