{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "\n",
    "This notebook transforms the raw eviction traces from policy model training (evictions.txt files) into a form that can provide meaningful information to the RAG implemented in RAG_application.ipynb. The final dataset is structured as a dictionary where each trace name is a key that indexes both a pandas dataframe containing numerical data on that trace and textual metadata, which includes overall performance metrics for the trace (e.g. hit rate) and descriptions of the policy model architecture and workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Generate Dataframe for a Single Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "def load_and_modify_data(file_path, max_cache_set_size, disassembly_file, source_directory):\n",
    "\n",
    "    def get_prefix(pc):\n",
    "        # pc is likely in form \"0x401a0f\", we extract \"401a0\"\n",
    "        return pc[2:2+5]\n",
    "\n",
    "    def find_function_code(func_name, directory):\n",
    "        # Regex to find where the function definition starts\n",
    "        pattern = re.compile(r'^\\s*[a-zA-Z_][a-zA-Z0-9_*\\s]*\\b' + re.escape(func_name) + r'\\s*\\(', re.MULTILINE)\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.c') or filename.endswith('.h'):\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                    # Find function start matches\n",
    "                    for match in pattern.finditer(content):\n",
    "                        start_index = match.start()\n",
    "                        brace_pos = content.find('{', start_index)\n",
    "                        if brace_pos == -1:\n",
    "                            continue\n",
    "\n",
    "                        # Count braces to determine function end\n",
    "                        brace_count = 0\n",
    "                        end_pos = brace_pos\n",
    "                        for i, ch in enumerate(content[brace_pos:], start=brace_pos):\n",
    "                            if ch == '{':\n",
    "                                brace_count += 1\n",
    "                            elif ch == '}':\n",
    "                                brace_count -= 1\n",
    "                                if brace_count == 0:\n",
    "                                    end_pos = i\n",
    "                                    break\n",
    "\n",
    "                        function_code = content[start_index:end_pos+1].strip()\n",
    "                        return function_code\n",
    "        return None\n",
    "\n",
    "    # Read eviction data\n",
    "    rows = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                row = json.loads(line.strip())  # Parse each line as JSON\n",
    "                rows.append(row)\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # Skip malformed lines\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    df.rename(columns={\n",
    "        'pc': 'program_counter',\n",
    "        'address': 'memory_address',\n",
    "        'set_id': 'cache_set_id',\n",
    "        'cache_lines': 'current_cache_lines',\n",
    "        'access_history': 'recent_access_history',\n",
    "        'evict': 'evict',\n",
    "        'cache_line_scores': 'cache_line_eviction_scores'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Modify 'evict' column\n",
    "    df['evict'] = df['evict'].apply(lambda x: 'Cache Miss' if x else 'Cache Hit')\n",
    "\n",
    "    # Extract current cache line addresses\n",
    "    def extract_cache_line_addresses(cache_lines):\n",
    "        if isinstance(cache_lines, list) and len(cache_lines) > 0:\n",
    "            return [entry[0] for entry in cache_lines]  # Extract only the addresses\n",
    "        return []\n",
    "\n",
    "    df['current_cache_line_addresses'] = df['current_cache_lines'].apply(extract_cache_line_addresses)\n",
    "\n",
    "    # Preprocessing for recency and reuse distance\n",
    "    last_access_time = {}  \n",
    "    address_access_indices = {}\n",
    "    for idx, address in enumerate(df['memory_address']):\n",
    "        address_access_indices.setdefault(address, []).append(idx)\n",
    "\n",
    "    # Build next access time mapping\n",
    "    next_access_time_mapping = {}\n",
    "    for address, indices in address_access_indices.items():\n",
    "        for i, current_idx in enumerate(indices):\n",
    "            if i + 1 < len(indices):\n",
    "                next_idx = indices[i + 1]\n",
    "                reuse_distance = next_idx - current_idx\n",
    "            else:\n",
    "                reuse_distance = 'not accessed again'\n",
    "            next_access_time_mapping[current_idx] = reuse_distance\n",
    "\n",
    "    # Mapping from cache_set_id to indices\n",
    "    cache_set_access_indices = {}\n",
    "    for idx, cache_set_id in enumerate(df['cache_set_id']):\n",
    "        cache_set_access_indices.setdefault(cache_set_id, []).append(idx)\n",
    "\n",
    "    evicted_addresses = []\n",
    "    accessed_address_recency = []\n",
    "    accessed_address_reuse_distance = []\n",
    "    evicted_address_reuse_distance = []\n",
    "    miss_types = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        memory_address = row['memory_address']\n",
    "        cache_set_id = row['cache_set_id']\n",
    "        evict_status = row['evict']  \n",
    "        current_cache_lines = row['current_cache_line_addresses']\n",
    "\n",
    "        # Determine recency\n",
    "        if memory_address in last_access_time:\n",
    "            recency = idx - last_access_time[memory_address]\n",
    "        else:\n",
    "            recency = 'First Access'\n",
    "        accessed_address_recency.append(recency)\n",
    "\n",
    "        # Update last access time\n",
    "        last_access_time[memory_address] = idx\n",
    "\n",
    "        # Get reuse distance for accessed address\n",
    "        reuse_distance = next_access_time_mapping.get(idx, 'not accessed again')\n",
    "        accessed_address_reuse_distance.append(reuse_distance)\n",
    "\n",
    "        evicted_address = None\n",
    "        evicted_reuse_distance = None\n",
    "        miss_type = None\n",
    "\n",
    "        if evict_status == 'Cache Miss':\n",
    "            # Determine miss type\n",
    "            num_cache_lines = len(current_cache_lines) if isinstance(current_cache_lines, list) else 0\n",
    "            if num_cache_lines >= max_cache_set_size:\n",
    "                miss_type = 'Capacity Miss'\n",
    "            else:\n",
    "                miss_type = 'Conflict Miss'\n",
    "\n",
    "            cache_set_indices = cache_set_access_indices.get(cache_set_id, [])\n",
    "            current_idx_position = cache_set_indices.index(idx)\n",
    "            if current_idx_position + 1 < len(cache_set_indices):\n",
    "                next_idx = cache_set_indices[current_idx_position + 1]\n",
    "                next_cache_lines = df.at[next_idx, 'current_cache_line_addresses']\n",
    "                evicted_candidates = set(current_cache_lines) - set(next_cache_lines) if next_cache_lines else set()\n",
    "                if evicted_candidates:\n",
    "                    evicted_address = evicted_candidates.pop()\n",
    "            else:\n",
    "                evicted_address = None\n",
    "\n",
    "            # Evicted address reuse distance\n",
    "            if evicted_address:\n",
    "                future_accesses = address_access_indices.get(evicted_address, [])\n",
    "                future_indices = [i for i in future_accesses if i > idx]\n",
    "                if future_indices:\n",
    "                    evicted_reuse_distance = future_indices[0] - idx\n",
    "                else:\n",
    "                    evicted_reuse_distance = 'not accessed again'\n",
    "        else:\n",
    "            miss_type = None\n",
    "\n",
    "        evicted_addresses.append(evicted_address)\n",
    "        evicted_address_reuse_distance.append(evicted_reuse_distance)\n",
    "        miss_types.append(miss_type)\n",
    "\n",
    "    # Add new columns\n",
    "    df['evicted_address'] = evicted_addresses\n",
    "    df['accessed_address_recency'] = accessed_address_recency\n",
    "    df['accessed_address_reuse_distance'] = accessed_address_reuse_distance\n",
    "    df['evicted_address_reuse_distance'] = evicted_address_reuse_distance\n",
    "    df['miss_type'] = miss_types\n",
    "\n",
    "    # Parse disassembly and track functions\n",
    "    disassembly_dict = {}\n",
    "    current_function = None\n",
    "    prefix = None\n",
    "\n",
    "    function_header_regex = re.compile(r'^([0-9a-fA-F]{16}) <([^>]+)>:$')\n",
    "    address_line_regex = re.compile(r'^\\s*([0-9a-fA-F]{6}):')\n",
    "\n",
    "    with open(disassembly_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            func_match = function_header_regex.match(line)\n",
    "            if func_match:\n",
    "                current_function = func_match.group(2)\n",
    "                continue\n",
    "            addr_match = address_line_regex.match(line)\n",
    "            if addr_match:\n",
    "                address_hex = addr_match.group(1)\n",
    "                prefix = address_hex[:5]\n",
    "                if prefix not in disassembly_dict:\n",
    "                    disassembly_dict[prefix] = {\n",
    "                        \"lines\": [],\n",
    "                        \"function_name\": current_function\n",
    "                    }\n",
    "                disassembly_dict[prefix][\"lines\"].append(line)\n",
    "            else:\n",
    "                if line.strip() and prefix and prefix in disassembly_dict:\n",
    "                    disassembly_dict[prefix][\"lines\"].append(line)\n",
    "\n",
    "    # Cache for function codes\n",
    "    function_code_cache = {}\n",
    "\n",
    "    # Add assembly_code, function_name, function_code columns\n",
    "    assembly_codes = []\n",
    "    function_names = []\n",
    "    function_codes = []\n",
    "\n",
    "    for pc in df['program_counter']:\n",
    "        pfx = get_prefix(pc)\n",
    "        entry = disassembly_dict.get(pfx, {\"lines\": [], \"function_name\": None})\n",
    "        assembly_code = \"\\n\".join(entry[\"lines\"])\n",
    "        func_name = entry[\"function_name\"]\n",
    "\n",
    "        # Retrieve function code if we have a function name\n",
    "        func_code = None\n",
    "        if func_name:\n",
    "            if func_name in function_code_cache:\n",
    "                func_code = function_code_cache[func_name]\n",
    "            else:\n",
    "                func_code = find_function_code(func_name, source_directory)\n",
    "                function_code_cache[func_name] = func_code\n",
    "\n",
    "        assembly_codes.append(assembly_code)\n",
    "        function_names.append(func_name)\n",
    "        function_codes.append(func_code)\n",
    "\n",
    "    df['assembly_code'] = assembly_codes\n",
    "    df['function_name'] = function_names\n",
    "    df['function_code'] = function_codes\n",
    "\n",
    "    # Reorder columns for better readability (optional)\n",
    "    fixed_columns = [\n",
    "        'program_counter', 'memory_address', 'cache_set_id', 'evict',\n",
    "        'miss_type', 'evicted_address', 'accessed_address_recency',\n",
    "        'accessed_address_reuse_distance', 'evicted_address_reuse_distance',\n",
    "        'function_name', 'function_code', 'assembly_code'\n",
    "    ]\n",
    "    remaining_cols = [c for c in df.columns if c not in fixed_columns]\n",
    "    df = df[fixed_columns + remaining_cols]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Load and Process Multiple Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_generate_metadata(file_paths, cache_set_sizes, policy_descriptions, workload_descriptions, disassembly_files, source_files):\n",
    "    \"\"\"\n",
    "    Processes multiple cache trace files and generates metadata and descriptions for each.\n",
    "    \n",
    "    Args:\n",
    "    - file_paths (list of str): List of file paths to the cache trace files.\n",
    "    - cache_set_sizes (list of int): List of max cache set sizes for each file path.\n",
    "    - policy_descriptions (list of str): List of replacement policy descriptions for each file.\n",
    "    - workload_descriptions (list of str): List of workload descriptions for each file.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where each key is a file name (without extension) and each value is a dictionary\n",
    "            containing the DataFrame, metadata, and combined description for each file.\n",
    "    \"\"\"\n",
    "    processed_data = {}\n",
    "\n",
    "    for file_path, max_cache_set_size, policy_desc, workload_desc, disassembly, source in zip(file_paths, cache_set_sizes, policy_descriptions, workload_descriptions, disassembly_files, source_files):\n",
    "        # Extract the base name of the file without extension for a meaningful dictionary key\n",
    "        file_name = file_path.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        # Process the file to create the DataFrame\n",
    "        df_modified = load_and_modify_data(file_path, max_cache_set_size, disassembly, source)\n",
    "        \n",
    "        # Generate the metadata\n",
    "        metadata_text = generate_metadata(df_modified)\n",
    "        \n",
    "        # Combine the policy and workload descriptions\n",
    "        combined_description = f\"Replacement Policy: {policy_desc}\\nWorkload: {workload_desc}\"\n",
    "        \n",
    "        # Store the processed DataFrame, metadata, and description in the dictionary\n",
    "        processed_data[file_name] = {\n",
    "            \"data_frame\": df_modified,\n",
    "            \"metadata\": metadata_text,\n",
    "            \"description\": combined_description\n",
    "        }\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def generate_metadata(df_modified):\n",
    "    \"\"\"\n",
    "    Generates metadata text from the processed DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df_modified (DataFrame): The DataFrame for which metadata needs to be generated.\n",
    "\n",
    "    Returns:\n",
    "    - str: Metadata text summarizing cache performance.\n",
    "    \"\"\"\n",
    "    # 1. Overall Cache Miss Rate\n",
    "    total_accesses = len(df_modified)\n",
    "    total_misses = df_modified['evict'].value_counts().get('Cache Miss', 0)\n",
    "    miss_rate = (total_misses / total_accesses) * 100\n",
    "\n",
    "    # 2. Miss Type Distribution (Capacity vs. Conflict Misses)\n",
    "    misses_df = df_modified[df_modified['evict'] == 'Cache Miss']\n",
    "    miss_type_counts = misses_df['miss_type'].value_counts()\n",
    "    capacity_misses = miss_type_counts.get('Capacity Miss', 0)\n",
    "    conflict_misses = miss_type_counts.get('Conflict Miss', 0)\n",
    "    total_misses_count = capacity_misses + conflict_misses\n",
    "    capacity_miss_percentage = (capacity_misses / total_misses_count) * 100 if total_misses_count > 0 else 0\n",
    "    conflict_miss_percentage = (conflict_misses / total_misses_count) * 100 if total_misses_count > 0 else 0\n",
    "\n",
    "    # 3. Eviction Efficiency Metrics\n",
    "    reuse_distance_threshold = 1000  # Define threshold\n",
    "    max_reuse_distance = df_modified[['accessed_address_reuse_distance', 'evicted_address_reuse_distance']].replace('not accessed again', np.nan).dropna().astype(float).max().max()\n",
    "    max_reuse_distance = max_reuse_distance + 1000 if not np.isnan(max_reuse_distance) else reuse_distance_threshold + 1000\n",
    "\n",
    "    df_modified['evicted_address_reuse_distance_numeric'] = df_modified['evicted_address_reuse_distance'].replace('not accessed again', max_reuse_distance).fillna(max_reuse_distance).astype(float)\n",
    "    df_modified['accessed_address_reuse_distance_numeric'] = df_modified['accessed_address_reuse_distance'].replace('not accessed again', max_reuse_distance).fillna(max_reuse_distance).astype(float)\n",
    "\n",
    "    total_evictions = df_modified['evicted_address'].notna().sum()\n",
    "    long_reuse_evictions = df_modified[df_modified['evicted_address_reuse_distance_numeric'] > reuse_distance_threshold]\n",
    "    percentage_long_reuse_evictions = (len(long_reuse_evictions) / total_evictions) * 100 if total_evictions > 0 else 0\n",
    "\n",
    "    # 4. Correlation Between Recency and Cache Misses\n",
    "    df_modified['is_miss'] = df_modified['evict'].apply(lambda x: 1 if x == 'Cache Miss' else 0)\n",
    "    df_modified['accessed_address_recency_numeric'] = df_modified['accessed_address_recency'].replace('First Access', np.nan).astype(float)\n",
    "    valid_indices = df_modified['accessed_address_recency_numeric'].dropna().index\n",
    "    recency_data = df_modified.loc[valid_indices, 'accessed_address_recency_numeric']\n",
    "    is_miss_data = df_modified.loc[valid_indices, 'is_miss']\n",
    "    correlation = recency_data.corr(is_miss_data) if not recency_data.empty else np.nan\n",
    "\n",
    "    # 5. Evictions where the evicted address has a lower reuse distance than the accessed address\n",
    "    evictions_df = df_modified[df_modified['evicted_address'].notna()].copy()\n",
    "    evictions_with_lower_reuse = evictions_df[evictions_df['evicted_address_reuse_distance_numeric'] < evictions_df['accessed_address_reuse_distance_numeric']]\n",
    "    num_evictions_with_lower_reuse = len(evictions_with_lower_reuse)\n",
    "    percentage_evictions_with_lower_reuse = (num_evictions_with_lower_reuse / total_evictions) * 100 if total_evictions > 0 else 0\n",
    "\n",
    "    # Prepare the metadata text\n",
    "    metadata_text = f\"\"\"\n",
    "    Cache Performance Summary: {total_accesses} total accesses, {total_misses} total misses, {miss_rate:.2f}% miss rate,\n",
    "    {capacity_miss_percentage:.2f}% capacity misses, {conflict_miss_percentage:.2f}% conflict misses, {total_evictions} total evictions, \n",
    "    {num_evictions_with_lower_reuse} ({percentage_evictions_with_lower_reuse:.2f}%) wrong evictions where evicted line has lower reuse distance. \n",
    "    The correlation between accessed address recency and cache misses is {correlation:.2f}.\n",
    "    \"\"\"\n",
    "    return metadata_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set arguments and create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_paths = [\n",
    "#    './eviction_traces/bzip_evictions_mlp.txt',\n",
    "#    './eviction_traces/bzip_evictions_parrot.txt',\n",
    "#    './eviction_traces/bzip_evictions_lru.txt',\n",
    "#    './eviction_traces/astar_evictions_lru.txt',\n",
    "#    './eviction_traces/ibm_evictions_mlp.txt'\n",
    "#]\n",
    "file_paths = [\n",
    "    './evictions/astar_evictions_lru.txt',\n",
    "    './evictions/astar_evictions_belady.txt',\n",
    "    './evictions/astar_evictions_mlp.txt',\n",
    "    './evictions/astar_evictions_parrot.txt',\n",
    "    './evictions/lbm_evictions_lru.txt',\n",
    "    './evictions/lbm_evictions_belady.txt',\n",
    "    './evictions/lbm_evictions_mlp.txt',\n",
    "    './evictions/lbm_evictions_parrot.txt',\n",
    "    './evictions/mcf_evictions_lru.txt',\n",
    "    './evictions/mcf_evictions_belady.txt',\n",
    "    './evictions/mcf_evictions_mlp.txt',\n",
    "    './evictions/mcf_evictions_parrot.txt'\n",
    "#    './evictions/gems_evictions_parrot.txt',\n",
    "#    './evictions/lbm_evictions_parrot.txt',\n",
    "#    './evictions/leslie3d_evictions_parrot.txt',\n",
    "#    './evictions/milc_evictions_parrot.txt',\n",
    "]\n",
    "\n",
    "#cache_set_sizes = [16, 16, 16, 16, 16, 16, 16, 16, 16]  # Associativity for each trace #Why 9?\n",
    "cache_set_sizes = [16, 16, 16, 16, 16, 16, 16, 16, 16]  # Associativity for each trace\n",
    "\n",
    "#policy_descriptions = [\n",
    "#    \"MLP: learned policy using a simple multi-layer perceptron to approximate belady's optimal policy\",\n",
    "#    \"PARROT: learned policy using attention to approximate belady's optimal policy\",\n",
    "#    \"LRU: evict the least recently used cache line\",\n",
    "#    \"LRU: evict the least recently used cache line\",\n",
    "#    \"MLP: learned policy using a simple multi-layer perceptron to approximate belady's optimal policy\"\n",
    "#    \"\"\n",
    "#]\n",
    "policy_descriptions = [\n",
    "    \"LRU: evict the least recently used cache line\",\n",
    "    \"Belady: optimal replacement policy. Evict the cache line with highest reuse distance\",\n",
    "    \"MLP: learned policy using a simple multi-layer perceptron to approximate belady's optimal policy\",\n",
    "    \"PARROT: learned policy using attention to approximate belady's optimal policy\",\n",
    "    \"LRU: evict the least recently used cache line\",\n",
    "    \"Belady: optimal replacement policy. Evict the cache line with highest reuse distance\",\n",
    "    \"MLP: learned policy using a simple multi-layer perceptron to approximate belady's optimal policy\",\n",
    "    \"PARROT: learned policy using attention to approximate belady's optimal policy\",\n",
    "    \"LRU: evict the least recently used cache line\",\n",
    "    \"Belady: optimal replacement policy. Evict the cache line with highest reuse distance\",\n",
    "    \"MLP: learned policy using a simple multi-layer perceptron to approximate belady's optimal policy\",\n",
    "    \"PARROT: learned policy using attention to approximate belady's optimal policy\"\n",
    "#    \"PARROT: learned policy using attention to approximate belady's optimal policy\",\n",
    "#    \"\"\"\n",
    "#    \"PARROT: learned policy using attention to approximate belady's optimal policy\",\n",
    "#    \"PARROT: learned policy using attention to approximate belady's optimal policy\",\n",
    "#    \"PARROT: learned policy using attention to approximate belady's optimal policy\",\n",
    "#    \"\"\"\n",
    "    \"\"\n",
    "]\n",
    "    \n",
    "#workload_descriptions = [\n",
    "#    \"The bzip workload has six parts: two JPEGs, a binary, a tar file with source code, an HTML file, and an archive with both highly and minimally compressible files. Each is compressed and decompressed at three blocking factors, with the decompressed output compared to the original.\",\n",
    "#    \"The bzip workload has six parts: two JPEGs, a binary, a tar file with source code, an HTML file, and an archive with both highly and minimally compressible files. Each is compressed and decompressed at three blocking factors, with the decompressed output compared to the original.\",\n",
    "#    \"The bzip workload has six parts: two JPEGs, a binary, a tar file with source code, an HTML file, and an archive with both highly and minimally compressible files. Each is compressed and decompressed at three blocking factors, with the decompressed output compared to the original.\",\n",
    "#    \"astar is a 2D path-finding library used in game AI. It includes three A* algorithms: one for passable/non-passable maps, another modified for varied terrain and move speed, and a third for graph-based maps with regional relationships. It also includes functions for determining map regions.\",\n",
    "#    \"The lbm workload simulates fluid dynamics and involves large, regular memory accesses, creating high memory bandwidth demands. This pattern results in frequent cache misses and high cache eviction rates, stressing the cache hierarchy as it continually loads new data. Consequently, lbm heavily tests a cache’s ability to manage large data sets with low temporal locality, making it an effective benchmark for evaluating cache replacement policies.\"    \n",
    "#]\n",
    "workload_descriptions = [\n",
    "    \"astar is derived from a portable 2D path-finding library that is used in game's AI. This library implements three different path-finding algorithms: First is the well known A* algorithm for maps with passable and non-passable terrain types. Second is a modification of the A* path finding algorithm for maps with different terrain types and different move speed. Third is an implementation of A* algorithm for graphs. This is formed by map regions with neighborhood relationship. The library also includes pseudo-intellectual functions for map region determination.\",\n",
    "    \"astar is derived from a portable 2D path-finding library that is used in game's AI. This library implements three different path-finding algorithms: First is the well known A* algorithm for maps with passable and non-passable terrain types. Second is a modification of the A* path finding algorithm for maps with different terrain types and different move speed. Third is an implementation of A* algorithm for graphs. This is formed by map regions with neighborhood relationship. The library also includes pseudo-intellectual functions for map region determination.\",\n",
    "    \"astar is derived from a portable 2D path-finding library that is used in game's AI. This library implements three different path-finding algorithms: First is the well known A* algorithm for maps with passable and non-passable terrain types. Second is a modification of the A* path finding algorithm for maps with different terrain types and different move speed. Third is an implementation of A* algorithm for graphs. This is formed by map regions with neighborhood relationship. The library also includes pseudo-intellectual functions for map region determination.\",\n",
    "    \"astar is derived from a portable 2D path-finding library that is used in game's AI. This library implements three different path-finding algorithms: First is the well known A* algorithm for maps with passable and non-passable terrain types. Second is a modification of the A* path finding algorithm for maps with different terrain types and different move speed. Third is an implementation of A* algorithm for graphs. This is formed by map regions with neighborhood relationship. The library also includes pseudo-intellectual functions for map region determination.\",\n",
    "    \"lbm workload implements the so-called Lattice Boltzmann Method (LBM) to simulate incompressible fluids in 3D. It is the computationally most important part of a larger code which is used in the field of material science to simulate the behavior of fluids with free surfaces, in particluar the formation and movement of gas bubbles in metal foams. For benchmarking purposes and easy optimization for different architectures, the code makes extensive use of macros which hide the details of the data access.\",\n",
    "    \"lbm workload implements the so-called Lattice Boltzmann Method (LBM) to simulate incompressible fluids in 3D. It is the computationally most important part of a larger code which is used in the field of material science to simulate the behavior of fluids with free surfaces, in particluar the formation and movement of gas bubbles in metal foams. For benchmarking purposes and easy optimization for different architectures, the code makes extensive use of macros which hide the details of the data access.\",\n",
    "    \"lbm workload implements the so-called Lattice Boltzmann Method (LBM) to simulate incompressible fluids in 3D. It is the computationally most important part of a larger code which is used in the field of material science to simulate the behavior of fluids with free surfaces, in particluar the formation and movement of gas bubbles in metal foams. For benchmarking purposes and easy optimization for different architectures, the code makes extensive use of macros which hide the details of the data access.\",\n",
    "    \"lbm workload implements the so-called Lattice Boltzmann Method (LBM) to simulate incompressible fluids in 3D. It is the computationally most important part of a larger code which is used in the field of material science to simulate the behavior of fluids with free surfaces, in particluar the formation and movement of gas bubbles in metal foams. For benchmarking purposes and easy optimization for different architectures, the code makes extensive use of macros which hide the details of the data access.\",\n",
    "    \"mcf is a C-based benchmark derived from MCF, a program for single-depot vehicle scheduling in public transportation. It uses integer arithmetic to solve large-scale minimum-cost flow problems with a network simplex algorithm. The goal is to minimize fleet size and operational costs by scheduling timetabled trips efficiently. The benchmark employs MCF Version 1.2, integrating it with column generation to accelerate the solution process, relying heavily on pointer and integer arithmetic.\",\n",
    "    \"mcf is a C-based benchmark derived from MCF, a program for single-depot vehicle scheduling in public transportation. It uses integer arithmetic to solve large-scale minimum-cost flow problems with a network simplex algorithm. The goal is to minimize fleet size and operational costs by scheduling timetabled trips efficiently. The benchmark employs MCF Version 1.2, integrating it with column generation to accelerate the solution process, relying heavily on pointer and integer arithmetic.\",\n",
    "    \"mcf is a C-based benchmark derived from MCF, a program for single-depot vehicle scheduling in public transportation. It uses integer arithmetic to solve large-scale minimum-cost flow problems with a network simplex algorithm. The goal is to minimize fleet size and operational costs by scheduling timetabled trips efficiently. The benchmark employs MCF Version 1.2, integrating it with column generation to accelerate the solution process, relying heavily on pointer and integer arithmetic.\",\n",
    "    \"mcf is a C-based benchmark derived from MCF, a program for single-depot vehicle scheduling in public transportation. It uses integer arithmetic to solve large-scale minimum-cost flow problems with a network simplex algorithm. The goal is to minimize fleet size and operational costs by scheduling timetabled trips efficiently. The benchmark employs MCF Version 1.2, integrating it with column generation to accelerate the solution process, relying heavily on pointer and integer arithmetic.\"\n",
    "\n",
    "#    \"GemsFDTD is a 3D FDTD solver for Maxwell’s equations, used to compute the radar cross section (RCS) of a conducting object. It follows three steps: initialization, timestepping (99% of execution time), and post-processing. It employs the Yee scheme on a staggered grid, Huygens' surfaces for wave generation, and Uni-axial Perfectly Matched Layer (UPML) for boundary absorption. The RCS is computed using near-to-far-field transformation with Fast Fourier Transforms (FFT).\",\n",
    "#    \"\"\"\n",
    "#    \"lbm workload implements the so-called Lattice Boltzmann Method (LBM) to simulate incompressible fluids in 3D. It is the computationally most important part of a larger code which is used in the field of material science to simulate the behavior of fluids with free surfaces, in particluar the formation and movement of gas bubbles in metal foams. For benchmarking purposes and easy optimization for different architectures, the code makes extensive use of macros which hide the details of the data access.\",\n",
    "#    \"leslie3d benchmark is a Computational Fluid Dynamics solver based on LESlie3d, used to study turbulence phenomena like mixing and combustion. For CPU2006, it solves a temporal mixing layer problem, a key benchmark for turbulent mixing physics. It employs a finite-volume algorithm with MacCormack Predictor-Corrector time integration, achieving fourth-order spatial and second-order temporal accuracy. The benchmark version, 437.leslie3d, minimizes file I/O to focus on CPU and memory performance.\",\n",
    "#    \"MILC is a set of C-based codes developed by the MIMD Lattice Computation (MILC) collaboration for simulating SU(3) lattice gauge theory on parallel machines. It is widely used in DOE and NSF supercomputer centers. The SPEC CPU2006 benchmark 433.milc runs the serial version of the su3imp program, essential for optimizing parallel performance. It generates gauge fields for lattice gauge theory, enabling the study of quarks and gluons in quantum field theory.\"\n",
    "#    \"\"\"\n",
    "]\n",
    "\n",
    "#disassembly_files = [\n",
    "#    \"./workload_source/bzip2_src/bzip3_disassembly.txt\",\n",
    "#    \"./workload_source/bzip2_src/bzip3_disassembly.txt\",\n",
    "#    \"./workload_source/bzip2_src/bzip3_disassembly.txt\",\n",
    "#    \"./workload_source/astar_src/astar_disassembly.txt\",\n",
    "#    \"./workload_source/lbm_src/lbm_disassembly.txt\"\n",
    "#]\n",
    "disassembly_files = [\n",
    "    \"./workload_disassembly/astar.txt\",\n",
    "    \"./workload_disassembly/astar.txt\",\n",
    "    \"./workload_disassembly/astar.txt\",\n",
    "    \"./workload_disassembly/astar.txt\",\n",
    "    \"./workload_disassembly/lbm.txt\",\n",
    "    \"./workload_disassembly/lbm.txt\",\n",
    "    \"./workload_disassembly/lbm.txt\",\n",
    "    \"./workload_disassembly/lbm.txt\",\n",
    "    \"./workload_disassembly/mcf.txt\",\n",
    "    \"./workload_disassembly/mcf.txt\",\n",
    "    \"./workload_disassembly/mcf.txt\",\n",
    "    \"./workload_disassembly/mcf.txt\"\n",
    "#    \"\"\"\n",
    "#    \"./workload_disassembly/lbm/lbm.txt\",\n",
    "#    \"./workload_disassembly/leslie3d/leslie3d.txt\",\n",
    "#    \"./workload_disassembly/milc/milc.txt\"\n",
    "#    \"\"\"\n",
    "]\n",
    "\n",
    "#source_files = [\n",
    "#    \"./workload_source/bzip2_src\",\n",
    "#    \"./workload_source/bzip2_src\",\n",
    "#    \"./workload_source/bzip2_src\",\n",
    "#    \"./workload_source/astar_src\",\n",
    "#    \"./workload_source/lbm_src\"\n",
    "#]\n",
    "source_files = [\n",
    "    \"./workload_source/astar\",\n",
    "    \"./workload_source/astar\",\n",
    "    \"./workload_source/astar\",\n",
    "    \"./workload_source/astar\",\n",
    "    \"./workload_source/lbm\",\n",
    "    \"./workload_source/lbm\",\n",
    "    \"./workload_source/lbm\",\n",
    "    \"./workload_source/lbm\",\n",
    "    \"./workload_source/mcf\",\n",
    "    \"./workload_source/mcf\",\n",
    "    \"./workload_source/mcf\",\n",
    "    \"./workload_source/mcf\"\n",
    "#    \"\"\"\n",
    "#    \"./workload_source/lbm\",\n",
    "#    \"./workload_source/leslie3d\",\n",
    "#    \"./workload_source/milc\"\n",
    "#    \"\"\"\n",
    "]\n",
    "\n",
    "# Process all files and store the results\n",
    "processed_data = process_and_generate_metadata(file_paths, cache_set_sizes, policy_descriptions, workload_descriptions, disassembly_files, source_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save processed_data to a file\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load processed_data from a file\n",
    "with open('processed_data.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# loaded_data now contains the same structure and data as processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some rows\n",
    "\n",
    "#Adjust display options to avoid truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Cache Performance Summary: 140704 total accesses, 92843 total misses, 65.98% miss rate,\\n    100.00% capacity misses, 0.00% conflict misses, 92802 total evictions, \\n    57057 (61.48%) wrong evictions where evicted line has lower reuse distance. \\n    The correlation between accessed address recency and cache misses is 0.34.\\n    '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample some metadata\n",
    "#loaded_data['bzip_evictions_mlp']['metadata']\n",
    "loaded_data['astar_evictions_parrot']['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Finetuning Dataset - Only if finetuning LLaMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_llm_training_data_from_dataframe(processed_data):\n",
    "    \"\"\"\n",
    "    Generates training data for an LLM from a unified DataFrame that includes metadata and descriptions.\n",
    "    \n",
    "    Args:\n",
    "    - processed_data (dict): Dictionary where each key is a file name and each value includes the DataFrame, metadata, and description.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of dictionaries with 'prompt' and 'completion' pairs for LLM training.\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    for file_name, file_info in processed_data.items():\n",
    "        df = file_info[\"data_frame\"]\n",
    "        policy_description = file_info[\"description\"].split(\"\\n\")[0]  # Replacement policy description\n",
    "        workload_description = file_info[\"description\"].split(\"\\n\")[1]  # Workload description\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Create a prompt based on the requested fields\n",
    "            prompt = (\n",
    "                f\"{policy_description}\\n\"\n",
    "                f\"{workload_description}\\n\"\n",
    "                f\"Program Counter: {row['program_counter']}\\n\"\n",
    "                f\"Memory Address: {row['memory_address']}\\n\"\n",
    "                f\"Cache State: {row['current_cache_line_addresses']}\\n\"\n",
    "                f\"Access History: {row['recent_access_history']}\\n\"\n",
    "                f\"Assembly Code: {row['assembly_code']}\\n\"\n",
    "                f\"Source Function: {row['function_name']}\\n\"\n",
    "            )\n",
    "\n",
    "            # Create the expected completion based on the requested fields\n",
    "            completion = (\n",
    "                f\"Evict: {row['evict']}\\n\"\n",
    "                f\"Miss Type: {row['miss_type'] if pd.notna(row['miss_type']) else 'N/A'}\\n\"\n",
    "                f\"Evicted Address: {row['evicted_address'] if pd.notna(row['evicted_address']) else 'N/A'}\\n\"\n",
    "                f\"Cache Line Scores: {row['cache_line_eviction_scores']}\\n\"\n",
    "            )\n",
    "\n",
    "            # Append the prompt and completion as a dictionary\n",
    "            training_data.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"completion\": completion\n",
    "            })\n",
    "\n",
    "    return training_data\n",
    "\n",
    "# Example usage\n",
    "llm_training_data = generate_llm_training_data_from_dataframe(loaded_data)\n",
    "\n",
    "# Optionally, save to a JSON file for training\n",
    "with open('llm_training_data_with_program_semantics.json', 'w') as f:\n",
    "    json.dump(llm_training_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
