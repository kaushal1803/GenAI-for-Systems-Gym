{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download LLM - LLaMa 3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74690eea2a2a4d16b45237b227623127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/csc591s25/kmhapse/tmp/ipykernel_3230764/2001596762.py:32: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  llm = HuggingFacePipeline(pipeline=pipeline)\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from huggingface_hub import login\n",
    "import transformers\n",
    "import torch\n",
    "import accelerate\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Script that will download llama 8B from hugging face. Run prior to testing the RAG.\n",
    "\n",
    "\n",
    "# Log in to Hugging Face using your access token\n",
    "#access_token = \"\"\n",
    "#login(token=access_token,add_to_git_credential=True)  \n",
    "\n",
    "# Define the model ID and create the pipeline using Hugging Face\n",
    "#model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "local_model = \"llama\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=local_model, #model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"auto\",\n",
    "    temperature=0.3,  # Control randomness (lower values = more focused responses)\n",
    "    top_p=0.9,  # Filter unlikely words\n",
    "    truncation=True,\n",
    "    max_length=500\n",
    ")\n",
    "\n",
    "# Wrap the Hugging Face pipeline with LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm interested in \"The Black Hole\" Artwork. Could you please confirm its availability and price. Thank you.\n",
      "Hello, Here is a firm offer for The Black Hole by Robert Longo. Thank you to confirm its availability, price and shipping conditions. Thank you.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are an expert in computer architecture and your job is to answer questions given data that describes cache accesses on different memory traces with different replacement policies. Base your response on the following data and your knowledge of computer architecture. Keep in mind that the optimal cache replacement policy will evict the address that is needed again furthest in the future and keep addresses it needs sooner. Justify your answers based on the provided information For replacement policy MLP: learned policy using a simple multi-layer perceptron to approximate belady's optimal policy on memory address 0x35e798a637f with pc 0x403a85: On accessing the cache, it got a Cache Hit.. Answer the following question: For the cache access with PC 0x403a85 and address 0x35e798a637f on the bzip workload with PARROT replacement policy, does the cache hit or miss? The answer to the question is, \"\n",
    "\n",
    "prompt=\"Hello\"\n",
    "# Pass the prompt to the LLM directly using `.run()`\n",
    "result = llm.invoke(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
